{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db522ee7",
   "metadata": {},
   "source": [
    "41. Merging two datasets(inner and outer join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7d82e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id value1 value2\n",
      "0   2      B      X\n",
      "1   3      C      Y\n",
      "   id value1 value2\n",
      "0   1      A    NaN\n",
      "1   2      B      X\n",
      "2   3      C      Y\n",
      "3   4    NaN      Z\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({'id':[1,2,3], 'value1':['A','B','C']})\n",
    "df2 = pd.DataFrame({'id':[2,3,4], 'value2':['X','Y','Z']})\n",
    "\n",
    "# Inner join\n",
    "inner_merge = pd.merge(df1, df2, on='id', how='inner')\n",
    "\n",
    "# Outer join\n",
    "outer_merge = pd.merge(df1, df2, on='id', how='outer')\n",
    "\n",
    "print(inner_merge)\n",
    "print(outer_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef2918",
   "metadata": {},
   "source": [
    "42. Pivot table for total sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a05c92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Sales\n",
      "Category          \n",
      "Clothing       350\n",
      "Electronics    350\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'Category':['Electronics','Electronics','Clothing','Clothing'],\n",
    "    'Sales':[200,150,100,250]\n",
    "})\n",
    "\n",
    "pivot = pd.pivot_table(data, values='Sales', index='Category', aggfunc='sum')\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d771b1",
   "metadata": {},
   "source": [
    "43. Reshaping dataframe using melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586c953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  Subject  Score\n",
      "0   1     Math     90\n",
      "1   2     Math     80\n",
      "2   1  Science     85\n",
      "3   2  Science     75\n"
     ]
    }
   ],
   "source": [
    "wide = pd.DataFrame({\n",
    "    'id':[1,2],\n",
    "    'Math':[90,80],\n",
    "    'Science':[85,75]\n",
    "})\n",
    "\n",
    "long = pd.melt(wide, id_vars=['id'], var_name='Subject', value_name='Score')\n",
    "print(long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd6aca7",
   "metadata": {},
   "source": [
    "44. Detecting and handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ff9a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    1\n",
      "dtype: int64\n",
      "          A        B\n",
      "0  1.000000        x\n",
      "1  2.000000  missing\n",
      "2  2.333333        y\n",
      "3  4.000000        z\n",
      "     A  B\n",
      "0  1.0  x\n",
      "3  4.0  z\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'A':[1,2,None,4],\n",
    "    'B':['x',None,'y','z']\n",
    "})\n",
    "\n",
    "# Detect\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle: fill with default values\n",
    "df_filled = df.fillna({'A':df['A'].mean(), 'B':'missing'})\n",
    "print(df_filled)\n",
    "\n",
    "# Or drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37536ea5",
   "metadata": {},
   "source": [
    "45. Encoding categorical variables using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61bd06ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_Blue  Color_Green  Color_Red\n",
      "0       False        False       True\n",
      "1        True        False      False\n",
      "2       False         True      False\n",
      "3       False        False       True\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Color':['Red','Blue','Green','Red']})\n",
    "\n",
    "encoded = pd.get_dummies(df, columns=['Color'])\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b222c3",
   "metadata": {},
   "source": [
    "46. Converting column to datetime & time-based indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c99c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales    200\n",
      "Name: 2025-01-02 00:00:00, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Date':['2025-01-01','2025-01-02','2025-01-03'],\n",
    "    'Sales':[100,200,150]\n",
    "})\n",
    "\n",
    "# Convert to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Set as index\n",
    "df = df.set_index('Date')\n",
    "\n",
    "# Time-based indexing\n",
    "print(df.loc['2025-01-02'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7919ea0",
   "metadata": {},
   "source": [
    "47. Forward and backward fill on time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1532a35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Value\n",
      "Date             \n",
      "2025-01-01   10.0\n",
      "2025-01-02   10.0\n",
      "2025-01-03   10.0\n",
      "2025-01-04   20.0\n",
      "2025-01-05   20.0\n",
      "            Value\n",
      "Date             \n",
      "2025-01-01   10.0\n",
      "2025-01-02   20.0\n",
      "2025-01-03   20.0\n",
      "2025-01-04   20.0\n",
      "2025-01-05    NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample time-series\n",
    "ts = pd.DataFrame({\n",
    "    'Date': pd.date_range('2025-01-01', periods=5),\n",
    "    'Value':[10, None, None, 20, None]\n",
    "}).set_index('Date')\n",
    "\n",
    "# Forward fill\n",
    "ffill = ts.ffill()\n",
    "\n",
    "# Backward fill\n",
    "bfill = ts.bfill()\n",
    "\n",
    "print(ffill)\n",
    "print(bfill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6a4ca",
   "metadata": {},
   "source": [
    "48. Normalizing and standarizing numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f355370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Normalized  Standardized\n",
      "0       10        0.00     -1.414214\n",
      "1       20        0.25     -0.707107\n",
      "2       30        0.50      0.000000\n",
      "3       40        0.75      0.707107\n",
      "4       50        1.00      1.414214\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "df = pd.DataFrame({'Feature':[10,20,30,40,50]})\n",
    "\n",
    "# Normalize (0–1 scale)\n",
    "scaler = MinMaxScaler()\n",
    "df['Normalized'] = scaler.fit_transform(df[['Feature']])\n",
    "\n",
    "# Standardize (mean=0, std=1)\n",
    "std_scaler = StandardScaler()\n",
    "df['Standardized'] = std_scaler.fit_transform(df[['Feature']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1193bcd",
   "metadata": {},
   "source": [
    "49. Reducing memory usage of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c738731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype   \n",
      "---  ------     --------------  -----   \n",
      " 0   int_col    10000 non-null  int16   \n",
      " 1   float_col  10000 non-null  float32 \n",
      " 2   cat_col    10000 non-null  category\n",
      "dtypes: category(1), float32(1), int16(1)\n",
      "memory usage: 68.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'int_col': np.random.randint(0,1000, size=10000),\n",
    "    'float_col': np.random.rand(10000),\n",
    "    'cat_col': np.random.choice(['A','B','C'], size=10000)\n",
    "})\n",
    "\n",
    "# Convert int to smaller dtype\n",
    "df['int_col'] = df['int_col'].astype('int16')\n",
    "\n",
    "# Convert float to smaller dtype\n",
    "df['float_col'] = df['float_col'].astype('float32')\n",
    "\n",
    "# Convert object to category\n",
    "df['cat_col'] = df['cat_col'].astype('category')\n",
    "\n",
    "print(df.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abe6c9",
   "metadata": {},
   "source": [
    "50. Processing large CSV file using chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63dada7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 with 2000 rows\n",
      "Chunk average: 50.1175\n",
      "Processing chunk 2 with 2000 rows\n",
      "Chunk average: 50.434\n",
      "Processing chunk 3 with 2000 rows\n",
      "Chunk average: 50.9055\n",
      "Processing chunk 4 with 2000 rows\n",
      "Chunk average: 49.1885\n",
      "Processing chunk 5 with 2000 rows\n",
      "Chunk average: 50.4755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Simulate a large dataset\n",
    "rows = 10000\n",
    "df_large = pd.DataFrame({\n",
    "    \"id\": range(rows),\n",
    "    \"value\": np.random.randint(1, 100, size=rows)\n",
    "})\n",
    "\n",
    "# Save to a temporary CSV\n",
    "df_large.to_csv(\"simulated_large.csv\", index=False)\n",
    "\n",
    "# Read in chunks\n",
    "chunk_iter = pd.read_csv(\"simulated_large.csv\", chunksize=2000)\n",
    "\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "    print(f\"Processing chunk {i+1} with {len(chunk)} rows\")\n",
    "    print(\"Chunk average:\", chunk[\"value\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f055e64",
   "metadata": {},
   "source": [
    "51. Reusable data cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dace14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      int_col  float_col cat_col\n",
      "0         953   0.466697       B\n",
      "1         938   0.500477       A\n",
      "2         794   0.122961       B\n",
      "3         571   0.707239       A\n",
      "4         238   0.082230       C\n",
      "...       ...        ...     ...\n",
      "9995      425   0.195040       A\n",
      "9996      362   0.201111       A\n",
      "9997      432   0.159121       A\n",
      "9998      797   0.520129       A\n",
      "9999      656   0.883197       A\n",
      "\n",
      "[10000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    # Fill missing numeric with mean\n",
    "    for col in df.select_dtypes(include='number'):\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "    # Fill missing categorical with mode\n",
    "    for col in df.select_dtypes(include='object'):\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cebaf",
   "metadata": {},
   "source": [
    "52. Implementing simple data-cleaning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356f96b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A        B\n",
      "0  1.000000        x\n",
      "1  2.000000  missing\n",
      "2  2.333333        y\n",
      "3  4.000000        z\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'A':[1,2,None,4],\n",
    "    'B':['x',None,'y','z']\n",
    "})\n",
    "\n",
    "# Pipeline steps\n",
    "df = df.drop_duplicates()\n",
    "df['A'] = df['A'].fillna(df['A'].mean())\n",
    "df['B'] = df['B'].fillna('missing')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19ebb6",
   "metadata": {},
   "source": [
    "53. Ingesting data from CSV,database,and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ee899bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  id                                              title  \\\n",
      "0       1   1  sunt aut facere repellat provident occaecati e...   \n",
      "1       1   2                                       qui est esse   \n",
      "2       1   3  ea molestias quasi exercitationem repellat qui...   \n",
      "3       1   4                               eum et est occaecati   \n",
      "4       1   5                                 nesciunt quas odio   \n",
      "\n",
      "                                                body  \n",
      "0  quia et suscipit\\nsuscipit recusandae consequu...  \n",
      "1  est rerum tempore vitae\\nsequi sint nihil repr...  \n",
      "2  et iusto sed quo iure\\nvoluptatem occaecati om...  \n",
      "3  ullam et saepe reiciendis voluptatem adipisci\\...  \n",
      "4  repudiandae veniam quaerat sunt sed\\nalias aut...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Example API (JSONPlaceholder)\n",
    "response = requests.get(\"https://jsonplaceholder.typicode.com/posts\")\n",
    "api_df = pd.DataFrame(response.json())\n",
    "\n",
    "print(api_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da762fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Data:\n",
      "    id   name\n",
      "0   1  Honey\n",
      "1   2  Bunny\n",
      "DB Data:\n",
      "    id   name\n",
      "0   1  Alice\n",
      "1   2    Bob\n",
      "2   1  Alice\n",
      "3   2    Bob\n",
      "4   1  Alice\n",
      "API Data:\n",
      "    userId  id                                              title  \\\n",
      "0       1   1  sunt aut facere repellat provident occaecati e...   \n",
      "1       1   2                                       qui est esse   \n",
      "2       1   3  ea molestias quasi exercitationem repellat qui...   \n",
      "3       1   4                               eum et est occaecati   \n",
      "4       1   5                                 nesciunt quas odio   \n",
      "\n",
      "                                                body  \n",
      "0  quia et suscipit\\nsuscipit recusandae consequu...  \n",
      "1  est rerum tempore vitae\\nsequi sint nihil repr...  \n",
      "2  et iusto sed quo iure\\nvoluptatem occaecati om...  \n",
      "3  ullam et saepe reiciendis voluptatem adipisci\\...  \n",
      "4  repudiandae veniam quaerat sunt sed\\nalias aut...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "\n",
    "# 1. CSV ingestion\n",
    "csv_df = pd.DataFrame({'id':[1,2], 'name':['Honey','Bunny']})\n",
    "csv_df.to_csv('data.csv', index=False)\n",
    "csv_df = pd.read_csv('data.csv')\n",
    "\n",
    "# 2. Database ingestion (SQLite)\n",
    "conn = sqlite3.connect('example.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS sample (id INTEGER, name TEXT)\")\n",
    "cursor.execute(\"INSERT INTO sample VALUES (1,'Alice'), (2,'Bob')\")\n",
    "conn.commit()\n",
    "\n",
    "db_df = pd.read_sql_query(\"SELECT * FROM sample\", conn)\n",
    "\n",
    "# 3. API ingestion (real endpoint)\n",
    "response = requests.get(\"https://jsonplaceholder.typicode.com/posts\")\n",
    "api_df = pd.DataFrame(response.json())\n",
    "\n",
    "print(\"CSV Data:\\n\", csv_df.head())\n",
    "print(\"DB Data:\\n\", db_df.head())\n",
    "print(\"API Data:\\n\", api_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf81732",
   "metadata": {},
   "source": [
    "54. Merging multiple datasets into a unified table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5173aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    name   age city\n",
      "0   1    Ross   NaN   NY\n",
      "1   2  Rachel  25.0  NaN\n",
      "2   3     NaN  30.0   LA\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'id':[1,2], 'name':['Ross','Rachel']})\n",
    "df2 = pd.DataFrame({'id':[2,3], 'age':[25,30]})\n",
    "df3 = pd.DataFrame({'id':[1,3], 'city':['NY','LA']})\n",
    "\n",
    "# Merge sequentially\n",
    "merged = df1.merge(df2, on='id', how='outer').merge(df3, on='id', how='outer')\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8919a3",
   "metadata": {},
   "source": [
    "55. Implementing RFM(Recency,Frequency,Monetary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83509228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Recency  Frequency  Monetary\n",
      "CustomerID                              \n",
      "1                21          2       300\n",
      "2                11          2       450\n",
      "3                16          1       400\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Sample transactions\n",
    "data = pd.DataFrame({\n",
    "    'CustomerID':[1,1,2,2,3],\n",
    "    'Date':[dt.date(2025,1,1), dt.date(2025,1,10),\n",
    "            dt.date(2025,1,5), dt.date(2025,1,20),\n",
    "            dt.date(2025,1,15)],\n",
    "    'Amount':[100,200,150,300,400]\n",
    "})\n",
    "\n",
    "# Reference date\n",
    "ref_date = dt.date(2025,1,31)\n",
    "\n",
    "rfm = data.groupby('CustomerID').agg({\n",
    "    'Date': lambda x: (ref_date - max(x)).days,   # Recency\n",
    "    'CustomerID': 'count',                        # Frequency\n",
    "    'Amount': 'sum'                               # Monetary\n",
    "})\n",
    "\n",
    "rfm.rename(columns={'Date':'Recency','CustomerID':'Frequency','Amount':'Monetary'}, inplace=True)\n",
    "print(rfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038fc29",
   "metadata": {},
   "source": [
    "56. Computing CLV(Customer Lifetime Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d812b554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            AvgPurchaseValue  PurchaseFrequency  CustomerLifespan     CLV\n",
      "CustomerID                                                               \n",
      "1                 150.000000                  2                 3   900.0\n",
      "2                 233.333333                  3                 3  2100.0\n",
      "3                 400.000000                  1                 3  1200.0\n"
     ]
    }
   ],
   "source": [
    "# Simplified CLV: average purchase value × purchase frequency × customer lifespan\n",
    "transactions = pd.DataFrame({\n",
    "    'CustomerID':[1,1,2,2,2,3],\n",
    "    'Amount':[100,200,150,300,250,400]\n",
    "})\n",
    "\n",
    "clv = transactions.groupby('CustomerID').agg({\n",
    "    'Amount':'mean'\n",
    "}).rename(columns={'Amount':'AvgPurchaseValue'})\n",
    "\n",
    "clv['PurchaseFrequency'] = transactions.groupby('CustomerID').size()\n",
    "clv['CustomerLifespan'] = 3  # assume 3 years for demo\n",
    "clv['CLV'] = clv['AvgPurchaseValue'] * clv['PurchaseFrequency'] * clv['CustomerLifespan']\n",
    "\n",
    "print(clv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a70640",
   "metadata": {},
   "source": [
    "57. Storing processed data into CSV or database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5255b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "rfm.to_csv('rfm_results.csv', index=True)\n",
    "\n",
    "# Save to SQLite database\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('results.db')\n",
    "rfm.to_sql('rfm_table', conn, if_exists='replace', index=True)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311675cb",
   "metadata": {},
   "source": [
    "58. Designing simple end-to-end data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51fcf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def ingest_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "def transform(df):\n",
    "    df['Total'] = df.select_dtypes(include='number').sum(axis=1)\n",
    "    return df\n",
    "\n",
    "def store(df, db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    conn.close()\n",
    "\n",
    "# Pipeline execution\n",
    "raw = ingest_csv('data.csv')\n",
    "clean = clean_data(raw)\n",
    "transformed = transform(clean)\n",
    "store(transformed, 'pipeline_results.db', 'final_table')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
